---
title: "Auto-correlación"
author: |
        | Santiago Bohorquez Correa
        |
        | Universidad EAFIT
        | Escuela de Economía y Finanzas
output:
  revealjs::revealjs_presentation:
    css: EAFIT.css
    highlight: pygments
    center: true
    transition: slide
    reveal_options:
      slideNumber: true
---


#
<ul>
<li> Una de las principales características de las series de tiempo es la presencia de Auto-correlación o correlación serial.</li>
<li> Ustedes posiblemente están familiarizados con el concepto de correlación entre dos variables, la auto-correlación implica correlación entre la serie y su pasado.</li>

#
Si la secuencia aleatoria $\{x_t\}$ tiene media $E[x_t]=\mu_t$ la autocovarianza esta dada por:
    \begin{equation}
    \begin{matrix}
        cov[x_{t_1},x_{t_2}]  & = & E[(x_{t_1}-\mu_{t_1})(x_{t_2}-\mu_{t_2})] \\
                & = & E[(x_{t_1}x_{t_2})] - \mu_{t_1}\mu_{t_2} 
      \end{matrix}
    \end{equation}
    
## Autocovarianza

\begin{equation}
\begin{matrix}
E[(x_{t_1}-\mu_{t_1})(x_{t_2}-\mu_{t_2})]  =    E[(x_{t_1}x_{t_2} - x_{t_1}\mu_{t_2} -  \mu_{t_1}x_{t_2} +  \mu_{t_1}\mu_{t_2})] \\
 =  E[x_{t_1}x_{t_2}] - E[x_{t_1}\mu_{t_2}] - E[\mu_{t_1}x_{t_2}] + E[\mu_{t_1}\mu_{t_2}] \\
 =  E[x_{t_1}x_{t_2}] - E[x_{t_1}]\mu_{t_2} - \mu_{t_1}E[x_{t_2}] +  \mu_{t_1}\mu_{t_2} \\
 =  E[x_{t_1}x_{t_2}] - \mu_{t_1}\mu_{t_2} - \mu_{t_1}\mu_{t_2} + \mu_{t_1}\mu_{t_2} \\
 = E[x_{t_1}x_{t_2}] - \mu_{t_1}\mu_{t_2}
\end{matrix}
\end{equation}

# 
La autocovarianza de orden $j$  se define como;

\begin{equation}
\gamma_j = cov(x_{t},x_{t-j}) =  E[(x_{t}x_{t-j})] - \mu_{t}\mu_{t-j} 
\end{equation}


#

Ahora, definimos la auto-correlación como

\begin{equation}
    \rho_j = \frac{\gamma_j}{\gamma_0}
\end{equation}
 
La ventaja de usar la auto-correlación en vez de la auto-covarianza es que la auto-correlación siempre va a estar entre -1 y 1, dado que la covarianza siempre es igual o menor en valor absoluto que la varianza. 


#

Las auto-covarianzas y auto-correlaciones expuestas anteriormente son los valores poblacionales, estas pueden ser estimadas en una muestra de tamaño $T$ de la siguiente manera:
    \begin{align}
        \hat{\gamma_j} = &  \frac{1}{T} \sum^T_{t=j+1} (x_t - \bar{x}_{j+1:T})(x_{t-j} - \bar{x}_{1:T-j}) \\
        \hat{\rho_j} = &  \frac{\hat{\gamma_j}}{\widehat{var(x_t)}}
    \end{align}



#
<ul>
<li> donde $\bar{x}_{j+1:T}$ es el promedio muestral de $x_t$ usando las observaciones $t=j+1,j+2,\dots,T$ </li>
<li> $\widehat{var(x_t)}=\frac{1}{T-1}\Sigma_{i=1}^T(x_i-\bar{x})^2$ es la varianza muestral de $x_t$.  </li>
<li> Noten que la ecuación de la auto-covarianza se divide por $T$ y no $T-j$ como es usual cuando perdemos grados de libertad, la razón para esto es para poder hacer comparaciones entre diferentes ordenes de las auto-covarianzas.</li>


#
Vemos un ejemplo de una serie de tiempo con auto-correlación:

El crecimiento del PIB trimestral para EEUU.

# 
<img src="img/gdpgrowth.png" alt="Ejemplo: Crecimiento PIB" width="900" height="450">  

#
<ul>
<li> En nuestro ejemplo de crecimiento del PIB las autocorrelaciones muestrales son, $\hat{\rho_1}=0.34$, $\hat{\rho_2}=0.27$, $\hat{\rho_3}=0.13$, $\hat{\rho_4}=0.14$.</li>
<li> Estos valores sugieren que el crecimiento del PIB esta levemente correlacionado positivamente.</li>
<li> Esto implica que si el PIB crece más (menos) que el promedio en un periodo, este tiende a crecer más (menos) que el promedio el periodo siguiente.</li>
</ul>

#

<ul>
<li> Claramente, si la autocorrelación es distinta de cero los supuestos de MCO no se cumplen.</li>
<li> Por esta razón se han escrito innumerables artículos de investigación sobre como testear por correlación serial en modelos de regresión.</li>
<li> La forma más sencilla de hacerlo es testeando la hipótesis nula de no correlación serial versus la alternativa de correlación serial de algún orden.</li>
</ul>

# Test correlación serial 


#

Supongamos el siguiente modelo de regresión lineal,
    \begin{equation}
        y_t = \mathbf{X}_t \boldsymbol{\beta} + u_t, \quad u_t = \rho u_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim IID(0,\sigma^2_{\varepsilon})
    \end{equation}

es decir, el termino  de error $u_t$ sigue un proceso AR(1).

Como veremos cuando analicemos los modelos Autorregresivos debemos imponer la restricción $|\rho|<1$.

#


Sustituyendo $u_t$ por $\rho u_{t-1} + \varepsilon_t$,

\begin{equation}
    y_t = \mathbf{X}_t \boldsymbol{\beta} +  \rho u_{t-1} + \varepsilon_t
\end{equation}
    
 Y remplazando $u_{t-1}$ por $y_{t-1} - \mathbf{X}_{t-1} \boldsymbol{\beta}$,
    \begin{equation}
        y_t = \rho y_{t-1} + \mathbf{X}_t \boldsymbol{\beta} - \rho\mathbf{X}_{t-1} \boldsymbol{\beta} + \varepsilon_t
    \end{equation}    


#

<ul>
<li> Como la variable dependiente rezagada, $y_{t-1}$, entra dentro de los regresores nos referimos a este modelo como un modelo <em> dinámico </em>.</li>
<li> Para este modelo perdemos el primer dato de la muestra ya que en este caso no conoceríamos el rezago para dicho momento.</li>
<li> Este modelo es lineal en los regresores, pero no es lineal en los parámetros, y por lo tanto debería ser estimado por Mínimos Cuadrados no lineales.</li>


# Test correlación serial - Gauss Newton

#

Podemos evitar hacer esta estimación no lineal, si hacemos un test basándonos en la regresión de Gauss-Newton.

Primero estimamos por MCO la regresión:
    \begin{equation}
        y_t =  \mathbf{X}_t \boldsymbol{\beta} + u_t
    \end{equation}  
    

#

Sea $\tilde{u}$ el vector de residuales de la regresión anterior.

Con esto, el test consiste en probar la hipótesis nula de $b_p = 0$ en la regresión,
    \begin{equation}
        \tilde{u}_t = b_p \tilde{u}_{t-1} + residuales
    \end{equation}
    donde $b_p \sim N(0,1)$ bajo la hipótesis nula.

# Test correlación serial - Otros Tests

#
<b>Yule-Walker Test:</b> Calculamos la autocorrelación de los residuales,
    \begin{equation}
        \hat{\rho}_1 = \frac{\hat{\gamma}_1}{\hat{\gamma}_0}
    \end{equation}
    testeamos $\hat{\rho}_1 = 0$ con $Var(\hat{\rho}_1) = \frac{1}{T}$.
